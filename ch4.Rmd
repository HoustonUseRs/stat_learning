---
title: "Chapter 4"
output: html_notebook
---


```{r}
library(pacman)

# stat libs
p_load(MASS, ISLR)

# data cleaning
p_load(broom, dplyr, janitor)

# plots
p_load(corrplot, ggplot2)
```


## Q 10

### cleaning column names
```{r}
weekly = Weekly %>% clean_names()
```

**trend in volume**

```{r}
plot(weekly$volume)
```


**examine the structure of the data**

```{r}
head(weekly)
str(weekly)
summary(weekly)
```



**quickly examine trends among variables**

```{r, fig.height=8, fig.width=8}
pairs(weekly)
```


```{r}
cor(weekly[, -9])
corrplot(cor(Weekly[, -9]))
```

Let us fit a logistic model to begin with:

### b
```{r}
glm_fit_a = glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + volume, 
              data = weekly, family = "binomial")
tidy(glm_fit_a)
summary(glm_fit_a)

# no intercept
glm_fit_b = glm(direction ~ 0 + lag1 + lag2 + lag3 + 
                 lag4 + lag5 + volume, data = weekly, 
               family = binomial)
summary(glm_fit_b)

# changing the sequence of variables
glm_fit_c = glm(direction ~ lag2 + lag3 + 
                 lag4 + lag5 + volume + lag1, 
                data = weekly, 
               family = binomial)
summary(glm_fit_c)
```


**number of weeks when marker was up/down**

```{r}
ggplot(weekly, aes(x = as.factor(year), fill = direction)) +
  geom_bar()
```

```{r}
# Extract coefficients
tidy(coef(glm_fit))
summary(glm_fit$coefficients) # another way to extract coeff.
```

`type` tell glm to output prob in form of P(Y = 1|X)

response: on the scale of Y
default: log-odds (default predictions are of log-odds (probabilities on logit scale)

```{r}
# Use predict function
glm.probs = predict(glm_fit, type = "response") 
# type tell glm to output prob in form of P(Y = 1|X)
tidy(glm.probs[1:10])
Smarket[1:10,]
glm_fit$fitted.values[1:10]

df = data.frame(obs_dir = Smarket[,"Direction"], 
                fitted = glm_fit$fitted.values, stringsAsFactors = FALSE)
df
```

```{r}
ggplot(df, aes(Actual, fitted)) + geom_boxplot() + geom_jitter()
```

```{r}
df = mutate(df, fitted_dir = ifelse(fitted > 0.5, "Up", "Down"))
```

```{r}
filter(df, obs_dir == "Down", fitted_dir == "Up")

table(df$obs_dir, df$fitted_dir)
```


Rows represent first variable.
We correctly predict Down as Down 145 times


```{r}
(507 + 145)/1250
```


Training error rate: `r 100-52.16`.

Section LDA

```{r}
p_load(MASS)

train = (Smarket$Year < 2005)

lda_fit = lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
print(lda_fit)
```

$\beta_1$: -0.642
$\beta_2$: -0.513

```{r}
plot(lda_fit)
```



```{r}
smarket_2005 = Smarket [!train, ]
pred_lda = predict(lda_fit, newdata = smarket_2005)

class_lda = pred_lda$class

table(class_lda, smarket_2005$Direction)

mean(class_lda == smarket_2005$Direction)

```


#' KNN

KNN does not like formulas, it needs 


```{r}
p_load(class)

# X: predictors
train.X = cbind(Smarket$Lag1, Smarket$Lag2)[train ,]
test.X = cbind(Smarket$Lag1, Smarket$Lag2)[!train, ]

# Y: response
train.Direction = Smarket$Direction[train]



for(k in 1:4){
  set.seed(1)
  cat("working with k:", k)
  #           training  test    what is the response
  knn.pred = knn(train.X, test.X, train.Direction, k = )
  table(knn.pred, smarket_2005$Direction)
  mean(knn.pred == smarket_2005$Direction)
}

```



Using LDA with MLR

```{r}
p_load(mlr)
data(iris)

print(iris)

## Define the task
task = makeClassifTask(id = "tutorial", data = iris, target = "Species")

## Define the learner
lrn = makeLearner("classif.lda")

## Define the resampling strategy
rdesc = makeResampleDesc(method = "CV", stratify = TRUE)

## Do the resampling
r = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE)

## Get the mean misclassification error
print(r)

```


```{r}
library(MASS)
fit.lda <- lda(Weekly$direction ~ lag2, 
               data = Weekly, 
               subset = train)
fit.lda

pred.lda <- predict(fit.lda, Weekly.20092010)
table(pred.lda$class, direction.20092010)
```


## Q 11.
```{r}
attach(Auto)
mpg01 <- rep(0, length(mpg))
mpg01[mpg > median(mpg)] <- 1
Auto <- data.frame(Auto, mpg01)
```


```{r}
cor(Auto[, -9])
pairs(Auto)
```

```{r}
boxplot(cylinders ~ mpg01, data = Auto, main = "Cylinders vs mpg01")
boxplot(displacement ~ mpg01, data = Auto, main = "Displacement vs mpg01")
boxplot(horsepower ~ mpg01, data = Auto, main = "Horsepower vs mpg01")
boxplot(weight ~ mpg01, data = Auto, main = "Weight vs mpg01")
boxplot(acceleration ~ mpg01, data = Auto, main = "Acceleration vs mpg01")
boxplot(year ~ mpg01, data = Auto, main = "Year vs mpg01")

```

c We may conclude that there exists some association between “mpg01” and “cylinders”, “weight”, “displacement” and “horsepower”.
Split the data into a training set and a test set.

```{r}
train = (year%%2 == 0)  # if the year is even
test = !train
Auto.train = Auto[train, ]
Auto.test = Auto[test, ]
mpg01.test = mpg01[test]
```

d. Perform LDA on the training data in order to predict “mpg01” using the variables that seemed most associated with “mpg01” in (b). What is the test error of the model obtained ?

```{r}
p_load(MASS)

# LDA
library(MASS)
lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, 
              data = Auto, 
    subset = train)
lda.pred = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)
```

e. 

```{r}
# QDA
qda.fit = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
    subset = train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)
```

f. logistic

```{r}
# Logistic regression
glm.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
    family = binomial, subset = train)
glm.probs = predict(glm.fit, Auto.test, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```


g. KNN

```{r}
library(class)
train.X = cbind(cylinders, weight, displacement, horsepower)[train, ]
test.X = cbind(cylinders, weight, displacement, horsepower)[test, ]
train.mpg01 = mpg01[train]
set.seed(1)

# KNN(k=1)
knn.pred = knn(train.X, test.X, train.mpg01, k = 1)
mean(knn.pred != mpg01.test)

# KNN(k=10)
knn.pred = knn(train.X, test.X, train.mpg01, k = 10)
mean(knn.pred != mpg01.test)

# KNN(k=100)
knn.pred = knn(train.X, test.X, train.mpg01, k = 100)
mean(knn.pred != mpg01.test)

```

```{r}

```
# END